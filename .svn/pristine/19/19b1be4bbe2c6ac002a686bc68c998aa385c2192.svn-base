/**
 * 获取用户的所有的微博（默认选择所有原创微博）
 * 并将微博地址存在文件中
 * 微博的具体转发需要使用repost包进行抓取
 * @update time 2013.8.29
 * 
 */
package com.weibo.post;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.LinkedList;
import java.util.List;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.http.HttpEntity;
import org.apache.http.HttpResponse;
import org.apache.http.client.ClientProtocolException;
import org.apache.http.client.HttpClient;
import org.apache.http.client.methods.HttpGet;

import com.weibo.common.Common;
import com.weibo.common.FileProcess;
import com.weibo.common.PageGet;
import com.weibo.login.Login;

/**
 * @author Jeffee
 * 
 */
public class AllPost {

	public static void main(String[] args) {
		HttpClient client = Login.login("social_ck@sina.com","jeffee");
		AllPost all = new AllPost(client, "1005053545286555");
		all.getAllPosts(5);
		System.out.println("Finished!");
	}

	private String pageID;
	private HttpClient client;
	private String baseDir;
	private String maxID = "";
	private String endID = "";
	private List<String> urlList;
	private List<String> infoList;
	
	public AllPost(HttpClient client, String pageid) {
		this.pageID = pageid;
		this.client = client;
		baseDir = "E:\\Sina\\weibo\\" + pageid;
		File dir = new File(baseDir);
		if (!dir.exists() || !dir.isDirectory())
			dir.mkdirs();
		urlList = new LinkedList<String>();
		infoList = new LinkedList<String>();
	}

	public void getOriginPosts(int end) {
		getPosts(1,end,1);
	}
	
	public void getOriginPosts(int start, int end) {
		getPosts(start,end,1);
	}
	
	public void getAllPosts(int end){
		getPosts(1,end, 0);
	}
	public void getAllPosts(int start, int end){
		getPosts(start,end, 0);
	}
	
	/**
	 * 抓取所有微博
	 * @param count 抓取的页数
	 * 所有微博的url：
	 * "http://weibo.com/p/1005051195403385/weibo?from=page_100505_home&wvr=5.1&mod=weibomore"
	 * 原创微博的url
	 * ："http://weibo.com/p/1005051195403385/weibo?profile_ftype=1&is_ori=1#_0"
	 * 
	 * ***/
	private void getPosts(int start, int end, int origin) {
		for (int i = start; i <= end; i++) {
			getFirstParagraph(i,origin);
			getRest(i,origin);
		}
		FileProcess.write(baseDir+"\\allPosts", urlList);
		FileProcess.write(baseDir+"\\infos", infoList);
	}


	/** 抓取一页中的第一部分微博，（最初显示的15条） **/
	private void getFirstParagraph(int pageNum, int origin) {
		String url = "http://weibo.com/p/"
				+ pageID
				+ "/weibo?profile_ftype=0&page="+pageNum+"&is_ori="+origin+"&from=page_100505_home&wvr=5.1&mod=originalweibo#place";
		
		HttpGet get = new HttpGet(url);
		HttpResponse response;
		boolean hasFound = false;
		try {
			response = client.execute(get);
			HttpEntity entity = response.getEntity();
			BufferedReader br = new BufferedReader(new InputStreamReader(
					entity.getContent(), "utf8"));
			String line = br.readLine();
			while (line != null) {
				if (line.startsWith("<script>FM.view({\"ns\":\"pl.header.head.index")) {
					String fName = "E:\\Sina\\user\\" + pageID;
					FileProcess.write(fName, line); // 个人信息
				} else if (line
						.startsWith("<script>FM.view({\"ns\":\"pl.content.homeFeed.index")) {
					extract(line);
					String fileName = baseDir + "\\info-" + pageNum + "-1";
					FileProcess.write(fileName, line);
					hasFound = true;
					break;
				}
				line = br.readLine();
			}
			if(!hasFound)
				System.err.println(pageID+"  not found!");
			
			br.close();
			get.abort();
		} catch (ClientProtocolException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		} catch (IOException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}

	/**
	 * 抓取一页中的第二三部分微博，（需要加载显示的30条） //默认只显示页面的1/3内容，15条微博，需要再循环两次抓取
	 * //抓取时生成地址需要用到max_id和endid，其中max_id为显示的最后一条微博的mid
	 * //endid为显示的第一条微博的ID，即从endid开始计数，第15条显示的为max_id //同一次抓取，微博的endid都相同
	 * //地址类似于
	 * "http://weibo.com/p/aj/mblog/mbloglist?domain=100505&pre_page=2&page=2&max_id=3475339705570090&end_id=3476099474253588&count=15&pagebar=0&pl_name=Pl_Official_LeftProfileFeed__11&id=1005051195403385&script_uri=/p/1005051195403385/weibo&feed_type=0&is_search=0&visible=0&is_ori=1&is_tag=0&profile_ftype=1"
	 * 
	 * **/
	private void getRest(int pageNum,int origin) {
		int i = 2;
		while (i <= 3) {
			String url = "http://weibo.com/p/aj/mblog/mbloglist?domain=100505&pre_page="
					+ pageNum
					+ "&page="
					+ pageNum
					+ "&max_id="
					+ maxID
					+ "&end_id="
					+ endID
					+ "&count=15&pagebar="
					+ (i - 2)
					+ "&pl_name=Pl_Official_LeftProfileFeed__11&id="
					+ pageID
					+ "&script_uri=/p/"
					+ pageID
					+ "/weibo&feed_type=0&is_search=0&visible=0&is_ori="+origin+"&is_tag=0&profile_ftype=1";

			//System.out.println(url);
			String line = PageGet.getJasonPage(client, url);
			//System.out.println(line);
			extract(line);
			String fileName = baseDir + "\\info-" + pageNum + "-" + i++;
			FileProcess.write(fileName, line);
		}
	}

	/***
	 * 对微博内容进行分析，抽取其中的endID和maxID，并将微博地址加入到urlList中
	 * 其中endID只在第一次进行分析时赋值，以后保持不变（显示的第一条微博的mid） maxID随着抓取不断改变（最后显示的一条微博的mid）
	 * **/
	private void extract(String line) {
		//System.out.println(line);
		Pattern pattern = Pattern
				.compile("allowForward=1&url=(.*?)&mid=(\\d*)&.*?(?:\\\\u8f6c\\\\u53d1|转发)\\(?(.*?)\\)?<.*?(?:\\\\u6536\\\\u85cf|收藏)\\(?(.*?)\\)?<.*?(?:\\\\u8bc4\\\\u8bba|评论)\\(?(.*?)\\)?<");
		Matcher matcher = pattern.matcher(line);
		while (matcher.find()) {
			int repostCount = Common.getNum(matcher.group(3));
			int favoCount = Common.getNum(matcher.group(4));
			int commentCount = Common.getNum(matcher.group(5));
			String url = matcher.group(1).replace("\\", "");
			urlList.add(url);
			if (endID.equals(""))
				endID = matcher.group(2);
			maxID = matcher.group(2);
			String info = maxID+" "+pageID+" "+repostCount+" "+favoCount+" "+commentCount+" "+url;
			//System.out.println(info);
			infoList.add(info);
		}
	}
	

}
